{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import copy\n",
    "import time \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set default tensor type to float32\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, x_min, x_max):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        \n",
    "    def normalize(self, x):\n",
    "        return 2.0 * (x - self.x_min) / (self.x_max - self.x_min) - 1.0\n",
    "        \n",
    "    def denormalize(self, x_norm):\n",
    "        return 0.5 * (x_norm + 1.0) * (self.x_max - self.x_min) + self.x_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = nn.Linear(2, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.hidden3 = nn.Linear(128, 128)\n",
    "        self.hidden4 = nn.Linear(128, 128)  \n",
    "        self.hidden5 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 2)\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = torch.tanh(self.hidden4(x))\n",
    "        x = torch.tanh(self.hidden5(x))\n",
    "        return self.output(x)  # return [u, v]\n",
    "    # Define the problem domain using the given vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = np.array([\n",
    "    [0, 0],\n",
    "    [0, 4],\n",
    "    [5, 4],\n",
    "    [5, 0],\n",
    "    [0, 0], # Closing the polygon\n",
    "],dtype=np.float32)\n",
    "path = Path(vertices)\n",
    "def in_domain(x, y):\n",
    "    points = np.column_stack((x.cpu().numpy(), y.cpu().numpy()))\n",
    "    return torch.tensor(path.contains_points(points), dtype=torch.bool, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define boundary conditions with tolerance\n",
    "def BC_bottom(x, y):\n",
    "    tol = 1e-6\n",
    "    return ((torch.abs(y - 0) < tol) & (x >= 0) & (x <= 5)).squeeze()\n",
    "\n",
    "def BC_left(x, y):\n",
    "    tol = 1e-6\n",
    "    return ((torch.abs(x - 0) < tol) & (y >= 0) & (y <= 4)).squeeze()\n",
    "\n",
    "def BC_top(x, y):\n",
    "    tol = 1e-6\n",
    "    return ((torch.abs(y - 4) < tol) & (x >= 0) & (x <= 5)).squeeze()\n",
    "\n",
    "def BC_right(x, y):\n",
    "    tol = 1e-6\n",
    "    return ((torch.abs(x - 5) < tol) & (y >= 0) & (y <= 4)).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss:\n",
    "    def __init__(self, delta=1.0):\n",
    "        self.delta = delta\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        error = torch.abs(y_true - y_pred)\n",
    "        quadratic = torch.min(error, torch.tensor(self.delta))\n",
    "        linear = error - quadratic\n",
    "        return torch.mean(0.5 * quadratic ** 2 + self.delta * linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BC(xy, net):\n",
    "    x, y = xy[:, 0].unsqueeze(1), xy[:, 1].unsqueeze(1)\n",
    "    \n",
    "    output = net(xy)\n",
    "    u = output[:, 0:1]  # Split output channel (u)\n",
    "    v = output[:, 1:2]  # Split output channel (v)\n",
    "    # Get boundary conditions\n",
    "    \n",
    "    bc_b = BC_bottom(x, y)\n",
    "    bc_l = BC_left(x, y)\n",
    "    bc_t = BC_top(x, y)\n",
    "    bc_r = BC_right(x, y)\n",
    "    \n",
    "    huber = HuberLoss(delta=1.0)\n",
    "    \n",
    "    loss = huber(u[bc_b], torch.zeros_like(u[bc_b])) + huber(v[bc_b], torch.zeros_like(v[bc_b])) # ux = 0, uy = 0 at bottom\n",
    "    loss += huber(u[bc_l], torch.zeros_like(u[bc_l])) # ux = 0 at left\n",
    "    loss += huber(u[bc_r], torch.zeros_like(u[bc_r])) # ux = 0 at right\n",
    "    \n",
    "    # Process top boundary\n",
    "    xy_top = xy[bc_t].requires_grad_(True)\n",
    "    x_top = xy_top[:, 0:1]  # x-coordinates of points on the top boundary\n",
    "    \n",
    "    output_top = net(xy_top)\n",
    "    u_top = output_top[:, 0:1]\n",
    "    v_top = output_top[:, 1:2]\n",
    "    \n",
    "    u_x_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    u_y_top = torch.autograd.grad(u_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    v_x_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 0]\n",
    "    v_y_top = torch.autograd.grad(v_top.sum(), xy_top, create_graph=True)[0][:, 1]\n",
    "    \n",
    "    E = 15   # Young's modulus MPa\n",
    "    nu = 0.3  # Poisson's ratio\n",
    "    \n",
    "    # Calculate stresses\n",
    "    sigma_xx_top = (E / ((1 + nu) * (1 - 2 * nu))) * ((1 - nu) * u_x_top + nu * v_y_top)\n",
    "    sigma_yy_top = (E / ((1 + nu) * (1 - 2 * nu))) * (nu * u_x_top + (1 - nu) * v_y_top)\n",
    "    sigma_xy_top = E / (2 * (1 + nu)) * (u_y_top + v_x_top)\n",
    "    \n",
    "    # Create mask for points with distributed load (x from 0 to 1)\n",
    "    load_mask = (x_top >= 2) & (x_top <= 3)\n",
    "    load_mask = load_mask.squeeze()  # Remove extra dimension\n",
    "    \n",
    "    # All points on top have sigma_xy = 0\n",
    "    loss += huber(sigma_xy_top, torch.zeros_like(sigma_xy_top))\n",
    "    \n",
    "    # Add condition for the area without load on the top edge\n",
    "    no_load_mask = ~load_mask\n",
    "    if torch.any(no_load_mask):\n",
    " \n",
    "        loss += huber(sigma_yy_top[no_load_mask], torch.zeros_like(sigma_yy_top[no_load_mask]))\n",
    "    \n",
    "    # Points with load have sigma_yy = -1.5 (negative for compression)\n",
    "    if torch.any(load_mask):\n",
    "        loss += huber(sigma_yy_top[load_mask], -0.5 * torch.ones_like(sigma_yy_top[load_mask]))\n",
    "    \n",
    "    # Process right boundary\n",
    "    xy_right = xy[bc_r].requires_grad_(True)\n",
    "    output_right = net(xy_right)\n",
    "    u_right = output_right[:, 0:1]\n",
    "    v_right = output_right[:, 1:2]\n",
    "    \n",
    "    u_y_right = torch.autograd.grad(u_right.sum(), xy_right, create_graph=True)[0][:, 1]\n",
    "    v_x_right = torch.autograd.grad(v_right.sum(), xy_right, create_graph=True)[0][:, 0]\n",
    "    u_x_right = torch.autograd.grad(u_right.sum(), xy_right, create_graph=True)[0][:, 0]\n",
    "    v_y_right = torch.autograd.grad(v_right.sum(), xy_right, create_graph=True)[0][:, 1]\n",
    "    sigma_xy_right = E / (2 * (1 + nu)) * (u_y_right + v_x_right)\n",
    "    sigma_xx_right = (E / ((1 + nu) * (1 - 2 * nu))) * ((1 - nu) * u_x_right + nu * v_y_right)\n",
    "    loss += huber(sigma_xy_right, torch.zeros_like(sigma_xy_right)) # Sigma_xy = 0 at right\n",
    "    loss += huber(sigma_xx_right, torch.zeros_like(sigma_xx_right)) # Sigma_xy = 0 at right\n",
    "    # Process left boundary\n",
    "    xy_left = xy[bc_l].requires_grad_(True)\n",
    "    output_left = net(xy_left)\n",
    "    u_left = output_left[:, 0:1]\n",
    "    v_left = output_left[:, 1:2]\n",
    "    \n",
    "    u_y_left = torch.autograd.grad(u_left.sum(), xy_left, create_graph=True)[0][:, 1]\n",
    "    v_x_left = torch.autograd.grad(v_left.sum(), xy_left, create_graph=True)[0][:, 0]\n",
    "    \n",
    "    sigma_xy_left = E / (2 * (1 + nu)) * (u_y_left + v_x_left)\n",
    "    loss += huber(sigma_xy_left, torch.zeros_like(sigma_xy_left)) # Sigma_xy = 0 at left\n",
    "\n",
    "    # Process bottom boundary\n",
    "    xy_bottom = xy[bc_b].requires_grad_(True)\n",
    "    output_bottom = net(xy_bottom)\n",
    "    u_bottom = output_bottom[:, 0:1]\n",
    "    v_bottom = output_bottom[:, 1:2]\n",
    "    \n",
    "    u_y_bottom = torch.autograd.grad(u_bottom.sum(), xy_bottom, create_graph=True)[0][:, 1]\n",
    "    v_x_bottom = torch.autograd.grad(v_bottom.sum(), xy_bottom, create_graph=True)[0][:, 0]\n",
    "    u_x_bottom = torch.autograd.grad(u_bottom.sum(), xy_bottom, create_graph=True)[0][:, 0]\n",
    "    v_y_bottom = torch.autograd.grad(v_bottom.sum(), xy_bottom, create_graph=True)[0][:, 1]\n",
    "    sigma_xy_bottom = E / (2 * (1 + nu)) * (u_y_bottom + v_x_bottom)\n",
    "    sigma_xx_bottom = (E / ((1 + nu) * (1 - 2 * nu))) * ((1 - nu) * u_x_bottom + nu * v_y_bottom)\n",
    "    sigma_yy_bottom = (E / ((1 + nu) * (1 - 2 * nu))) * (nu * u_x_bottom + (1 - nu) * v_y_bottom)\n",
    "    loss += huber(sigma_xy_bottom, torch.zeros_like(sigma_xy_bottom)) # Sigma_xy = 0 at bottom\n",
    "    loss += huber(sigma_xx_bottom, torch.zeros_like(sigma_xx_bottom)) # Sigma_xx = 0 at bottom\n",
    "    loss += huber(sigma_yy_bottom, torch.zeros_like(sigma_yy_bottom)) # Sigma_xx = 0 at bottom\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, n_epochs, fem_data_file=None, nx=100, ny=100):\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience = 3000\n",
    "    \n",
    "    prev_net = None\n",
    "    \n",
    "    #Add lists for loss history\n",
    "    loss_history = []\n",
    "    pde_x_history = []\n",
    "    pde_y_history = []\n",
    "    bc_history = []\n",
    "    epoch_history = []\n",
    "    \n",
    "    # Create training points only once\n",
    "    print(\"Generating training points...\")\n",
    "    x_inner, y_inner = generate_point_distribution(nx=nx, ny=ny)\n",
    "    print(f\"Generated {len(x_inner)} interior points\")\n",
    "    \n",
    "    # Create boundary points only once\n",
    "    print(\"Generating boundary points...\")\n",
    "    num_boundary_points = 100 # Number of points on each boundary\n",
    "    \n",
    "    # Create points on the bottom edge (y=0)\n",
    "    x_bottom = torch.linspace(0, 5, num_boundary_points, device=device)\n",
    "    y_bottom = torch.zeros(num_boundary_points, device=device)\n",
    "    \n",
    "    # Create points on the left edge (x=0)\n",
    "    x_left = torch.zeros(num_boundary_points, device=device)\n",
    "    y_left = torch.linspace(0, 4, num_boundary_points, device=device)\n",
    "    \n",
    "    # Create points on the top edge (y=4)\n",
    "    x_top = torch.linspace(0, 5, num_boundary_points, device=device)\n",
    "    y_top = torch.ones(num_boundary_points, device=device) * 4\n",
    "    \n",
    "    # Create points on the right edge (x=5)\n",
    "    x_right = torch.ones(num_boundary_points, device=device) * 5\n",
    "    y_right = torch.linspace(0, 4, num_boundary_points, device=device)\n",
    "    \n",
    "    # Total boundary points\n",
    "    x_b = torch.cat([x_bottom, x_left, x_top, x_right])\n",
    "    y_b = torch.cat([y_bottom, y_left, y_top, y_right])\n",
    "    \n",
    "    # Create tensor for BC function\n",
    "    xy_b = torch.stack([x_b, y_b], dim=1)\n",
    "    print(f\"Generated {len(xy_b)} boundary points\")\n",
    "    \n",
    "    print(f\"Starting training for {n_epochs} epochs...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # using x_inner, y_inner\n",
    "        loss_pde_x, loss_pde_y = PDE(x_inner, y_inner, net)\n",
    "        loss_bc = BC(xy_b, net)\n",
    "        \n",
    "        # calculate total loss\n",
    "        loss = loss_pde_x + loss_pde_y + loss_bc\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        loss_history.append(loss.item())\n",
    "        pde_x_history.append(loss_pde_x.item())\n",
    "        pde_y_history.append(loss_pde_y.item())\n",
    "        bc_history.append(loss_bc.item())\n",
    "        epoch_history.append(epoch)\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            patience = 0\n",
    "            \n",
    "            prev_net = copy.deepcopy(net)\n",
    "            \n",
    "            torch.save({\n",
    "                'net_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': loss,\n",
    "                'epoch': epoch,\n",
    "            }, 'best_model.pth')\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > max_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.10f}, '\n",
    "                  f'PDE_x: {loss_pde_x.item():.10f}, PDE_y: {loss_pde_y.item():.10f}, '\n",
    "                  f'BC: {loss_bc.item():.10f}, LR: {scheduler.get_last_lr()[0]:.10f}')\n",
    "    \n",
    "    # Creat graph Loss history\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot total loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epoch_history, loss_history, 'b-', label='Total Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Training Loss History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot component losses\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epoch_history, pde_x_history, 'r-', label='PDE_x Loss')\n",
    "    plt.plot(epoch_history, pde_y_history, 'g-', label='PDE_y Loss')\n",
    "    plt.plot(epoch_history, bc_history, 'b-', label='BC Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Component Losses (log scale)')\n",
    "    plt.title('Component Losses History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    #additional return value for metrics for tracking training progress\n",
    "    metrics = {\n",
    "        'loss_history': loss_history,\n",
    "        'pde_x_history': pde_x_history,\n",
    "        'pde_y_history': pde_y_history,\n",
    "        'bc_history': bc_history,\n",
    "        'epoch_history': epoch_history\n",
    "    }\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    return net, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapoint sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_point_distribution(nx=100, ny=100, load_density=10, load_influence=70):\n",
    "    # Defined boundaries of the domain\n",
    "    x_min, x_max = 0.0, 5.0\n",
    "    y_min, y_max = 0.0, 4.0\n",
    "    \n",
    "    # Create random points in the domain\n",
    "    # Add number of points to be 4 times the grid size\n",
    "    num_random_points = nx * ny * 4\n",
    "    x_random = torch.rand(num_random_points, device=device) * (x_max - x_min) + x_min\n",
    "    y_random = torch.rand(num_random_points, device=device) * (y_max - y_min) + y_min\n",
    "    \n",
    "    print(f\"Total random points generated: {len(x_random)}\")\n",
    "    \n",
    "    load_x_start, load_x_end = 2.0, 3.0\n",
    "    load_y = 4.0\n",
    "    \n",
    "    distances = torch.zeros_like(x_random)\n",
    "    \n",
    "   \n",
    "    in_x_range = (x_random >= load_x_start) & (x_random <= load_x_end)\n",
    "    \n",
    "    distances[in_x_range] = torch.abs(y_random[in_x_range] - load_y)\n",
    "    \n",
    "     \n",
    "    left_of_range = x_random < load_x_start\n",
    "    \n",
    "    distances[left_of_range] = torch.sqrt(\n",
    "        (x_random[left_of_range] - load_x_start)**2 + \n",
    "        (y_random[left_of_range] - load_y)**2\n",
    "    )\n",
    "    \n",
    "    \n",
    "    right_of_range = x_random > load_x_end\n",
    "    \n",
    "    distances[right_of_range] = torch.sqrt(\n",
    "        (x_random[right_of_range] - load_x_end)**2 + \n",
    "        (y_random[right_of_range] - load_y)**2\n",
    "    )\n",
    "    \n",
    "    base_prob = 0.2 \n",
    "    additional_prob = load_influence / (1 + (distances * load_density)**2)\n",
    "    \n",
    "    \n",
    "    keep_prob = torch.clamp(base_prob + additional_prob, 0, 1)\n",
    "    \n",
    "    \n",
    "    keep_mask = torch.rand(num_random_points, device=device) < keep_prob\n",
    "    \n",
    "    \n",
    "    x_final = x_random[keep_mask]\n",
    "    y_final = y_random[keep_mask]\n",
    "    \n",
    "    print(f\"Final points after density-based filtering: {len(x_final)}\")\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    \n",
    "    plt.scatter(x_final.cpu().numpy(), y_final.cpu().numpy(), s=1, color='blue', alpha=0.7)\n",
    "    \n",
    "    \n",
    "    plt.plot([load_x_start, load_x_end], [load_y, load_y], 'r-', linewidth=2, label='Load line')\n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Point Distribution with Enhanced Load Area Density - {len(x_final)} points')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return x_final, y_final\n",
    "\n",
    "\n",
    "x_points, y_points = generate_point_distribution(nx=100, ny=100, load_density=10, load_influence=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDE(x, y, net):\n",
    "    # Reshape 1D tensors into column vectors for concatenation\n",
    "    x_reshaped = x.unsqueeze(1) if x.dim() == 1 else x\n",
    "    y_reshaped = y.unsqueeze(1) if y.dim() == 1 else y\n",
    "    \n",
    "    xy = torch.cat([x_reshaped, y_reshaped], dim=1)\n",
    "    xy.requires_grad = True\n",
    "    \n",
    "    output = net(xy)\n",
    "    u = output[:, 0:1]\n",
    "    v = output[:, 1:2]\n",
    "    \n",
    "    u_x = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    u_y = torch.autograd.grad(u.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    v_x = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    v_y = torch.autograd.grad(v.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    \n",
    "    E = 15  # Young's modulus MPa \n",
    "    nu = 0.3 # Poisson's ratio\n",
    "    gamma = 0.018 # Distributed load (Mn/m^2)\n",
    "    sigma_xx = E / (1 - nu**2) * (u_x + nu * v_y)\n",
    "    sigma_yy = E / (1 - nu**2) * (v_y + nu * u_x)\n",
    "    sigma_xy = E / (2 * (1 + nu)) * (u_y + v_x)\n",
    "    \n",
    "    f_x = torch.zeros_like(x_reshaped)\n",
    "    f_y = -gamma * torch.ones_like(y_reshaped)\n",
    "    \n",
    "    R_x = torch.autograd.grad(sigma_xx.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_x\n",
    "    R_y = torch.autograd.grad(sigma_xy.sum(), xy, create_graph=True)[0][:, 0].unsqueeze(1) + \\\n",
    "          torch.autograd.grad(sigma_yy.sum(), xy, create_graph=True)[0][:, 1].unsqueeze(1) + f_y\n",
    "    \n",
    "    huber = HuberLoss(delta=1.0)\n",
    "    loss_x = huber(R_x, torch.zeros_like(R_x))\n",
    "    loss_y = huber(R_y, torch.zeros_like(R_y))\n",
    "    \n",
    "    return loss_x, loss_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, n_epochs, fem_data_file=None, nx=100, ny=100):\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-6)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience = 3000\n",
    "    \n",
    "    prev_net = None\n",
    "    \n",
    "    \n",
    "    loss_history = []\n",
    "    pde_x_history = []\n",
    "    pde_y_history = []\n",
    "    bc_history = []\n",
    "    epoch_history = []\n",
    "    \n",
    "    \n",
    "    print(\"Generating training points...\")\n",
    "    x_inner, y_inner = generate_point_distribution(nx=nx, ny=ny)\n",
    "    print(f\"Generated {len(x_inner)} interior points\")\n",
    "    \n",
    "    \n",
    "    print(\"Generating boundary points...\")\n",
    "    num_boundary_points = 1000 \n",
    "    \n",
    "    \n",
    "    x_bottom = torch.linspace(0, 5, num_boundary_points, device=device)\n",
    "    y_bottom = torch.zeros(num_boundary_points, device=device)\n",
    "    \n",
    "    \n",
    "    x_left = torch.zeros(num_boundary_points, device=device)\n",
    "    y_left = torch.linspace(0, 4, num_boundary_points, device=device)\n",
    "    \n",
    "    \n",
    "    x_top = torch.linspace(0, 5, num_boundary_points, device=device)\n",
    "    y_top = torch.ones(num_boundary_points, device=device) * 4\n",
    "    \n",
    "    \n",
    "    x_right = torch.ones(num_boundary_points, device=device) * 5\n",
    "    y_right = torch.linspace(0, 4, num_boundary_points, device=device)\n",
    "    \n",
    "    \n",
    "    x_b = torch.cat([x_bottom, x_left, x_top, x_right])\n",
    "    y_b = torch.cat([y_bottom, y_left, y_top, y_right])\n",
    "    \n",
    "    \n",
    "    xy_b = torch.stack([x_b, y_b], dim=1)\n",
    "    print(f\"Generated {len(xy_b)} boundary points\")\n",
    "    \n",
    "    print(f\"Starting training for {n_epochs} epochs...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        loss_pde_x, loss_pde_y = PDE(x_inner, y_inner, net)\n",
    "        loss_bc = BC(xy_b, net)\n",
    "        \n",
    "        \n",
    "        loss = loss_pde_x + loss_pde_y + loss_bc\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "       \n",
    "        loss_history.append(loss.item())\n",
    "        pde_x_history.append(loss_pde_x.item())\n",
    "        pde_y_history.append(loss_pde_y.item())\n",
    "        bc_history.append(loss_bc.item())\n",
    "        epoch_history.append(epoch)\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            patience = 0\n",
    "            \n",
    "            prev_net = copy.deepcopy(net)\n",
    "            \n",
    "            torch.save({\n",
    "                'net_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': loss,\n",
    "                'epoch': epoch,\n",
    "            }, 'best_model.pth')\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > max_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.10f}, '\n",
    "                  f'PDE_x: {loss_pde_x.item():.10f}, PDE_y: {loss_pde_y.item():.10f}, '\n",
    "                  f'BC: {loss_bc.item():.10f}, LR: {scheduler.get_last_lr()[0]:.10f}')\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epoch_history, loss_history, 'b-', label='Total Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Training Loss History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epoch_history, pde_x_history, 'r-', label='PDE_x Loss')\n",
    "    plt.plot(epoch_history, pde_y_history, 'g-', label='PDE_y Loss')\n",
    "    plt.plot(epoch_history, bc_history, 'b-', label='BC Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Component Losses (log scale)')\n",
    "    plt.title('Component Losses History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "   \n",
    "    metrics = {\n",
    "        'loss_history': loss_history,\n",
    "        'pde_x_history': pde_x_history,\n",
    "        'pde_y_history': pde_y_history,\n",
    "        'bc_history': bc_history,\n",
    "        'epoch_history': epoch_history\n",
    "    }\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    return net, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(net, output_filename='Footing_data.csv'):\n",
    "    nx, ny = 100, 100\n",
    "    X = torch.linspace(0, 5.0, nx, device=device)  \n",
    "    Y = torch.linspace(0, 4.0, ny, device=device)  \n",
    "    \n",
    "    xx, yy = torch.meshgrid(X, Y, indexing='ij')\n",
    "    \n",
    "    X = xx.flatten()\n",
    "    Y = yy.flatten()\n",
    "    \n",
    "   \n",
    "    mask = in_domain(X, Y)\n",
    "    mask_np = mask.cpu().numpy()  \n",
    "    \n",
    "    \n",
    "    XY = torch.stack([X, Y], dim=1)\n",
    "    XY.requires_grad_(True)\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        output = net(XY)\n",
    "        U = output[:, 0:1]  \n",
    "        V = output[:, 1:2]  \n",
    "        \n",
    "        U_x = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        U_y = torch.autograd.grad(U.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        V_x = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 0]\n",
    "        V_y = torch.autograd.grad(V.sum(), XY, create_graph=True)[0][:, 1]\n",
    "        \n",
    "        E = 15  # Mpa Young's modulus\n",
    "        nu = 0.3  # Poisson's ratio\n",
    "        \n",
    "        sigma_xx = (E / ((1 + nu) * (1 - 2 * nu))) * ((1 - nu) * U_x + nu * V_y)\n",
    "        sigma_yy = (E / ((1 + nu) * (1 - 2 * nu))) * (nu * U_x + (1 - nu) * V_y)\n",
    "        sigma_xy = E / (2 * (1 + nu)) * (U_y + V_x)\n",
    "    \n",
    "    \n",
    "    X_np = X.detach().cpu().numpy()\n",
    "    Y_np = Y.detach().cpu().numpy()\n",
    "    U_full = U.detach().cpu().numpy().squeeze()\n",
    "    V_full = V.detach().cpu().numpy().squeeze()\n",
    "    sigma_xx_full = sigma_xx.detach().cpu().numpy().squeeze()*1000\n",
    "    sigma_yy_full = sigma_yy.detach().cpu().numpy().squeeze()*1000\n",
    "    sigma_xy_full = sigma_xy.detach().cpu().numpy().squeeze()*1000\n",
    "    \n",
    "    \n",
    "    magnitude = np.sqrt(U_full**2 + V_full**2)\n",
    "    \n",
    "    \n",
    "    df_out = pd.DataFrame({\n",
    "        'X': X_np[mask_np],\n",
    "        'Y': Y_np[mask_np],\n",
    "        'ux': U_full[mask_np],\n",
    "        'uy': V_full[mask_np],\n",
    "        'sigma_xx': sigma_xx_full[mask_np],\n",
    "        'sigma_yy': sigma_yy_full[mask_np],\n",
    "        'sigma_xy': sigma_xy_full[mask_np],\n",
    "        'magnitude': magnitude[mask_np]\n",
    "    })\n",
    "    \n",
    "    \n",
    "    df_out.to_csv(output_filename, index=False)\n",
    "    print(f\"Results saved to {output_filename}\")\n",
    "    \n",
    "    \n",
    "    mask_cpu = mask.cpu().numpy()\n",
    "    U_masked = np.ma.masked_array(U_full, mask=~mask_cpu)\n",
    "    V_masked = np.ma.masked_array(V_full, mask=~mask_cpu)\n",
    "    sigma_xx_masked = np.ma.masked_array(sigma_xx_full, mask=~mask_cpu)\n",
    "    sigma_yy_masked = np.ma.masked_array(sigma_yy_full, mask=~mask_cpu)\n",
    "    sigma_xy_masked = np.ma.masked_array(sigma_xy_full, mask=~mask_cpu)\n",
    "    magnitude_masked = np.ma.masked_array(magnitude, mask=~mask_cpu)\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), U_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Displacement ux (m)')\n",
    "    plt.title('Displacement in x-direction')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), V_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Displacement uy (m)')\n",
    "    plt.title('Displacement in y-direction')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), magnitude_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Displacement magnitude (m)')\n",
    "    plt.title('Displacement magnitude')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), sigma_xx_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Stress σxx (kPa)')\n",
    "    plt.title('Normal stress σxx')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "   \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), sigma_yy_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Stress σyy (kPa)')\n",
    "    plt.title('Normal stress σyy')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.pcolormesh(X_np.reshape(nx, ny), Y_np.reshape(nx, ny), sigma_xy_masked.reshape(nx, ny), shading='auto', cmap='jet')\n",
    "    plt.colorbar(label='Stress σxy (kPa)')\n",
    "    plt.title('Shear stress σxy')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results_plots.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Result plots saved to results_plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize network and optimizer\n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the network\n",
    "    trained_net, metrics = train(net=net, optimizer=optimizer, n_epochs=4000)\n",
    "    # Plot results\n",
    "    try:\n",
    "        plot_results(trained_net)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in plot_results: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
